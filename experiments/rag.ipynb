{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# system prompt\n",
    "system_prompt = \"You are responsible to answer questions related to energy forcasting and report generation. Keep it concise as possible\"\n",
    "\n",
    "# llm\n",
    "model_name = \"llama3-groq-tool-use\"\n",
    "llm = Ollama(model=model_name, request_timeout=120.0, system=system_prompt)\n",
    "\n",
    "# get embed\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "assistant: The energy demand for Boston is 1,000 MWh. Is there anything else you need assistance with?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=system_prompt\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Context: boston energy demand is 1000\" + \"| Query: What is the energy demand for boston?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def ingest_documents():\n",
    "    path = \"./documents\"\n",
    "    collection_name = \"documents\"\n",
    "    \n",
    "    reader = SimpleDirectoryReader(input_dir=path)\n",
    "    documents = reader.load_data()\n",
    "    \n",
    "    # get collection\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # ingest and create new index\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, llm=llm, embed_model=embed_model)\n",
    "\n",
    "    return index\n",
    "\n",
    "index = ingest_documents()\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference\n",
    "def predict_demand(location_name: str) -> int:\n",
    "    \"\"\"Predicts and returns the energy demand in mwh for the given location name\"\"\"\n",
    "    return 10000\n",
    "\n",
    "def get_weather(location_name: str) -> dict:\n",
    "    \"\"\"Returns the weather dictionary for a given location.\"\"\"\n",
    "    return \"precipiation:100, wind:24, temp:45\"\n",
    "\n",
    "predict_demand_tool = FunctionTool.from_defaults(fn=predict_demand, description=\"function which returns the energy demand given the name of the location\")\n",
    "get_weather_tool = FunctionTool.from_defaults(fn=get_weather, description=\"function which returns the weather given the name of the location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([predict_demand_tool, get_weather_tool], llm=llm, system_prompt=system_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 48de03f5-6cd6-46fa-80de-6f70fc6b59ec. Step input: What is the weather and energy demand prediction for Boston. Find similar trends from the past data for different subba-names. Use the tools\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: get_weather\n",
      "Action Input: {'location_name': 'Boston'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: precipiation:100, wind:24, temp:45\n",
      "\u001b[0m> Running step c5d853b6-ffa1-4b75-9e40-a7a0cebdfbe4. Step input: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: predict_demand\n",
      "Action Input: {'location_name': 'Boston'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 10000\n",
      "\u001b[0m> Running step bd51810e-c72d-42c1-a874-50cb5187687b. Step input: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step 08b89057-ccc0-464e-9788-0c1c4a60b075. Step input: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The weather in Boston is precipiation at 100, wind at 24, and temperature at 45. The predicted energy demand is 10,000 units.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the weather and energy demand prediction for Boston. Find similar trends from the past data for different subba-names. Use the tools\")\n",
    "\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=\"system\", content=system_prompt\n",
    "#     ),\n",
    "#     ChatMessage(role=\"user\", content=\"Context: boston energy demand is 1000\" + \"| Query: What is the energy demand for boston?\"),\n",
    "# ]\n",
    "\n",
    "# response = agent.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can summarize that the weather in Boston is currently precipitating with 100% probability, the wind speed is 24 km/h, and the temperature is 45°C. The predicted energy demand for this location is 10,000 units.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gemini-api in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from gemini-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0,>=2.28.0->gemini-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0,>=2.28.0->gemini-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0,>=2.28.0->gemini-api) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0,>=2.28.0->gemini-api) (2024.8.30)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (2.20.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (1.12.11)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (2.35.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (3.20.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (2.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-core->google-generativeai) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: six<2dev,>=1.13.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-python-client->google-generativeai) (1.16.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-python-client->google-generativeai) (3.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from pydantic->google-generativeai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.65.5)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.48.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajat\\anaconda3\\envs\\mlops-3.10\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Downloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
      "   ---------------------------------------- 0.0/760.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 760.0/760.0 kB 16.1 MB/s eta 0:00:00\n",
      "Installing collected packages: google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed google-ai-generativelanguage-0.6.10 google-generativeai-0.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gemini-api\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston is the capital of Massachusetts.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def generate_answer(query):\n",
    "    prompt = (\n",
    "        f\"Maintain a friendly and conversational tone. If the passage is irrelevant, feel free to ignore it.\\n\\n\"\n",
    "        f\"QUESTION: '{query}'\\n\"\n",
    "        f\"ANSWER:\"\n",
    "    )\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    genai.configure(api_key=\"AIzaSyD_ONv8qHNxBxLydhaIcBSQL7Hx6GlHuhI\")\n",
    "    answer = model.generate_content(prompt)\n",
    "\n",
    "    \n",
    "    return answer.text\n",
    "\n",
    "answer = generate_answer(query=\"capital of massachussets\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
